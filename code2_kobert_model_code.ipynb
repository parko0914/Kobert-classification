{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Xtwe5Kq-lU4"
      },
      "outputs": [],
      "source": [
        "#구글드라이브 연동\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tmlv7VE-f37"
      },
      "outputs": [],
      "source": [
        "# gpu 켜기\n",
        "import torch\n",
        "device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgJXT29eS6Cr"
      },
      "outputs": [],
      "source": [
        "# 저장 경로 미리 지정\n",
        "path = '/content/drive/MyDrive/nlp_c/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdwBTlG6wKQf"
      },
      "source": [
        "# **로그파일 연동 시키기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SW0UyrLsDlM"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "def make_logger(name=None):\n",
        "    #1 logger instance를 만든다.\n",
        "    logger = logging.getLogger(name)\n",
        "\n",
        "    #2 logger의 level을 가장 낮은 수준인 DEBUG로 설정해둔다.\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "\n",
        "    #3 formatter 지정\n",
        "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
        "    \n",
        "    #4 handler instance 생성\n",
        "    console = logging.StreamHandler()\n",
        "    file_handler = logging.FileHandler(filename=path + \"logs/correct_final.log\",\n",
        "                                       encoding = 'utf-8')\n",
        "    \n",
        "    #5 handler 별로 다른 level 설정\n",
        "    console.setLevel(logging.INFO)\n",
        "    file_handler.setLevel(logging.DEBUG)\n",
        "\n",
        "    #6 handler 출력 format 지정\n",
        "    console.setFormatter(formatter)\n",
        "    file_handler.setFormatter(formatter)\n",
        "\n",
        "    #7 logger에 handler 추가\n",
        "    logger.addHandler(console)\n",
        "    logger.addHandler(file_handler)\n",
        "\n",
        "    return logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga_wQahzsFMy"
      },
      "outputs": [],
      "source": [
        "logger = make_logger()\n",
        "\n",
        "logger.debug(\"debug logging\")\n",
        "logger.info(\"info logging\")\n",
        "logger.warning(\"warning logging\")\n",
        "logger.error(\"error logging\")\n",
        "logger.critical(\"critical logging\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMrFUhYp94rf"
      },
      "source": [
        "# **필요한 환경 다운 및 구축**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX5AkfxF-rOe"
      },
      "source": [
        "## 학습모델 패키지 다운 및 구축(KoBERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KS3ueemx-9pw"
      },
      "outputs": [],
      "source": [
        "#깃허브에서 KoBERT 파일 로드\n",
        "!pip install ipywidgets  # for vscode\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVIVVVEx-9px"
      },
      "outputs": [],
      "source": [
        "# 필요한 모듈 로딩\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "#kobert\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "#transformers\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS7GR_oj-9px"
      },
      "outputs": [],
      "source": [
        "#BERT 모델, Vocabulary 불러오기\n",
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVTVa70d-9Kr"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz6Emo9U4hsd"
      },
      "source": [
        "# **맞춤법 검사한 데이터 불러온 뒤 전처리(EDA 방법 포함)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4EzR0q_E45D"
      },
      "outputs": [],
      "source": [
        "# 맞춤법 검사 완료한 train 파일 불러오기\n",
        "train_d = pd.read_csv(path + 'final_data/' + 'correct_train_fin.csv', encoding = 'utf-8-sig')\n",
        "print(len(train_d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGa2RUw1NVAN"
      },
      "outputs": [],
      "source": [
        "# index 변경 -> 아래 모델 생성에서 index 오류 발생하여 0부터 re-indexing\n",
        "# predict 값 추출하고, 다시 원래 y 값으로 변환해주는 방식으로 해야할 듯\n",
        "# 소분류 dictionary\n",
        "y_dict = pd.DataFrame({'origin_y' : train_d['y'].unique()}).sort_values(by = 'origin_y')\n",
        "y_dict['y'] = np.arange(0, len(y_dict))\n",
        "y_dict = y_dict.astype('str')\n",
        "s_dict0 = y_dict.set_index('origin_y').to_dict()['y'] # 처음 y값을 모델 train 을 위해 re-indexing\n",
        "s_dict1 = y_dict.set_index('y').to_dict()['origin_y'] # 뒤에 예측값 다시 y 값으로 return 할 때 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCUH6OL19QbG"
      },
      "outputs": [],
      "source": [
        "# kobert 모델 학습을 위해 reindexing 한 dictionary 저장 -> 후에 모델 예측값 도출 후, 기존 y값으로 되돌리기 위함\n",
        "import pickle\n",
        "with open(path+ 'final_data/' + 's_dictionary', 'wb') as f:\n",
        "    pickle.dump(s_dict1, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MjWITTpbnZl"
      },
      "outputs": [],
      "source": [
        "train_d['y_s'] = train_d['y'].astype('str')\n",
        "train_d['label_s'] = train_d['y_s'].map(s_dict0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRiiDxKZ47Si"
      },
      "source": [
        "## EDA 부분"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GOQHgdD8C0m"
      },
      "outputs": [],
      "source": [
        "# text augmentation\n",
        "# pip install -U nltk\n",
        "import nltk; \n",
        "nltk.download('omw-1.4');\n",
        "# nltk.download('wordnet') # 영문 버전"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAznCdfNlGCy"
      },
      "outputs": [],
      "source": [
        "# eda 폴더 생성\n",
        "% cd /content/drive/MyDrive/nlp_c\n",
        "# !git clone https://github.com/jasonwei20/eda_nlp\n",
        "# !git clone https://github.com/catSirup/KorEDA\n",
        "# eda는 eda_nlp/code 폴더에, wordnet.pickle 은 eda_nlp 폴더로 이동시키고, 진행\n",
        "% cd eda_nlp/\n",
        "# 추가적으로 augment.py 64번째 항에 eda -> EDA로 변경해야 실행됨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFp-__4ypg8s"
      },
      "outputs": [],
      "source": [
        "s_class_n = pd.DataFrame(train_d['y_s'].value_counts().sort_values())\n",
        "# s_class_n.to_csv(path + 'testset_class.csv', index=False, encoding='EUC-KR')\n",
        "s_class = s_class_n[s_class_n['y_s'] < 500].index.tolist()\n",
        "len(s_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc7y7pq6rpmM"
      },
      "outputs": [],
      "source": [
        "# n수가 부족한 class aug_num 차등으로 증강(적은 순서대로 20, 10 ,5) -> 상대적으로 부족한 클래스 데이터 비율이 더 높아지는 것을 조금이라도 방지하고자 함\n",
        "s_class1 = s_class[:30]\n",
        "s_class2 = s_class[30:60]\n",
        "s_class3 = s_class[60:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTxdEnQzpp8M"
      },
      "outputs": [],
      "source": [
        "few_d1 = train_d[train_d['y_s'].isin(s_class1)]\n",
        "few_d2 = train_d[train_d['y_s'].isin(s_class2)]\n",
        "few_d3 = train_d[train_d['y_s'].isin(s_class3)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7Z2BpVVmD1X"
      },
      "outputs": [],
      "source": [
        "# n이 100개 이하인 클래스 뽑아서 augmentation 가능한 파일 형태로 만들어주기\n",
        "txt_aug_list = [str(a) + '\\t' + str(b) for a, b in zip(few_d1['label_s'], few_d1['clean_done'])]\n",
        "with open(path + 'final_data/' + 'text_aug_1.txt', 'w') as f:\n",
        "  f.write('\\n'.join(txt_aug_list) + '\\n')\n",
        "\n",
        "txt_aug_list = [str(a) + '\\t' + str(b) for a, b in zip(few_d2['label_s'], few_d2['clean_done'])]\n",
        "with open(path + 'final_data/' + 'text_aug_2.txt', 'w') as f:\n",
        "  f.write('\\n'.join(txt_aug_list) + '\\n')\n",
        "\n",
        "txt_aug_list = [str(a) + '\\t' + str(b) for a, b in zip(few_d3['label_s'], few_d3['clean_done'])]\n",
        "with open(path + 'final_data/' + 'text_aug_3.txt', 'w') as f:\n",
        "  f.write('\\n'.join(txt_aug_list) + '\\n')\n",
        "\n",
        "# input file 형식 -> txt 파일 내 한 행 당 label + \\t + text 형태로 들어간 파일 \n",
        "# SR: Synonym Replacement, 특정 단어를 유의어로 교체\n",
        "# RI: Random Insertion, 임의의 단어를 삽입\n",
        "# RS: Random Swap, 문장 내 임의의 두 단어의 위치를 바꿈\n",
        "# RD: Random Deletion: 임의의 단어를 삭제\n",
        "!python code/augment.py --input=/content/drive/MyDrive/nlp_c/final_data/text_aug_1.txt --output=/content/drive/MyDrive/nlp_c/final_data/test_aug_eda_1.txt --num_aug=20 --alpha_sr=0.1 --alpha_rd=0.2 --alpha_ri=0.1 --alpha_rs=0.0\n",
        "!python code/augment.py --input=/content/drive/MyDrive/nlp_c/final_data/text_aug_2.txt --output=/content/drive/MyDrive/nlp_c/final_data/test_aug_eda_2.txt --num_aug=10 --alpha_sr=0.1 --alpha_rd=0.2 --alpha_ri=0.1 --alpha_rs=0.0\n",
        "!python code/augment.py --input=/content/drive/MyDrive/nlp_c/final_data/text_aug_3.txt --output=/content/drive/MyDrive/nlp_c/final_data/test_aug_eda_3.txt --num_aug=5 --alpha_sr=0.1 --alpha_rd=0.2 --alpha_ri=0.1 --alpha_rs=0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf-SrRdFsx8B"
      },
      "outputs": [],
      "source": [
        "# augmentation 완료한 데이터 불러와서 기존데이터셋에 붙여주기(augmentation 대상 데이터는 삭제)\n",
        "with open('/content/drive/MyDrive/nlp_c/final_data/test_aug_eda_1.txt', \"r\") as file:\n",
        "  strings = file.readlines()\n",
        "aug_d1 = pd.DataFrame([x.split('\\n')[0].split('\\t') for x in strings])\n",
        "aug_d1.columns = ['label_s', 'clean_done']\n",
        "\n",
        "with open('/content/drive/MyDrive/nlp_c/final_data/test_aug_eda_2.txt', \"r\") as file:\n",
        "  strings = file.readlines()\n",
        "aug_d2 = pd.DataFrame([x.split('\\n')[0].split('\\t') for x in strings])\n",
        "aug_d2.columns = ['label_s', 'clean_done']\n",
        "\n",
        "with open('/content/drive/MyDrive/nlp_c/final_data/test_aug_eda_3.txt', \"r\") as file:\n",
        "  strings = file.readlines()\n",
        "aug_d3 = pd.DataFrame([x.split('\\n')[0].split('\\t') for x in strings])\n",
        "aug_d3.columns = ['label_s', 'clean_done']\n",
        "\n",
        "train_d = train_d[train_d['y_s'].isin(s_class)==False]\n",
        "sample_d = pd.concat([train_d[['label_s', 'clean_done']], aug_d1, aug_d2, aug_d3], axis = 0, ignore_index = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tGRqywXa_3s"
      },
      "outputs": [],
      "source": [
        "sample_d['len'] = sample_d['clean_done'].astype(str).apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4C2yyKnuk2N"
      },
      "outputs": [],
      "source": [
        "i = 798000\n",
        "sample_d[i:i+50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB5a-U8Luk0G"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku0LfNbE4Ssn"
      },
      "source": [
        "# **처리한 데이터 kobert 모델에 학습**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "squG_otY9Qw4"
      },
      "outputs": [],
      "source": [
        "# train & test set 나누기\n",
        "from sklearn.model_selection import train_test_split\n",
        "# dataset_train, dataset_test = train_test_split(train_d, test_size=0.2, shuffle=True, random_state=30)\n",
        "# dataset_test.to_csv('/content/drive/MyDrive/nlp_c/testset.csv', index=False, encoding = 'EUC-KR')\n",
        "\n",
        "# stratify 를 target으로 지정해 비율을 맞춤으로써, 성능향상 가능\n",
        "# but, 현재 target 변수 class 비율의 불균형으로 오류 발생(1,2 개짜리 class 다수 존재)\n",
        "dataset_train, dataset_test, y_train, y_test = train_test_split(sample_d['clean_done'],\n",
        "                               sample_d['label_s'], random_state=132, stratify=sample_d['label_s']) \n",
        "# 모델 검증용 미리 뽑아놓기\n",
        "dataset_test.to_csv(path + 'final_data/' + 'testset.csv', index=False, encoding = 'utf-8-sig')\n",
        "y_test.to_csv(path + 'final_data/' + 'testset_y.csv', index=False, encoding = 'utf-8-sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mS5e9cD-C5K"
      },
      "outputs": [],
      "source": [
        "dataset_train = [[str(a), str(b)] for a, b in zip(dataset_train, y_train)]\n",
        "dataset_test = [[str(a), str(b)] for a, b in zip(dataset_test, y_test)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Awv8bqft-fcN"
      },
      "outputs": [],
      "source": [
        "print(len(dataset_train))\n",
        "print(len(dataset_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-Y_qoT0cOR8"
      },
      "outputs": [],
      "source": [
        "print(dataset_train[:10])\n",
        "print(dataset_test[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiT6xO5D-fji"
      },
      "outputs": [],
      "source": [
        "# KoBERT 입력 데이터로 만들기\n",
        "# BERT 모델에 들어가기 위한 dataset을 만들어주는 클래스\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlsWZhBv_9Dc"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8V_GHMa-fnx"
      },
      "outputs": [],
      "source": [
        "# Setting parameters\n",
        "# 이건 나중에 최적화 값 찾아봐야할 듯\n",
        "max_len = 64\n",
        "batch_size = 128\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate = 5e-5 # 0.0001 # \n",
        "\n",
        "# 토큰화 실행\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGY8r6HT-fsd"
      },
      "outputs": [],
      "source": [
        "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, pad = True, pair = False)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, pad = True, pair = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8w1sEhyDoTJ8"
      },
      "outputs": [],
      "source": [
        "sentence = dataset_train[30][0] \n",
        "print(sentence)\n",
        "print(tok(sentence))\n",
        "\n",
        "# 토큰화 패딩 처리 후 결과값 \n",
        "print(data_train[30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9N8PG6K_YO1"
      },
      "outputs": [],
      "source": [
        "# torch 형식의 dataset 생성\n",
        "# num_worker 은 gpu 활정화 정도, 5로 하니 오히려 과부화가 걸려 4로 조정\n",
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=4)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTPUgdQY_xvV"
      },
      "outputs": [],
      "source": [
        "# 클래스 수 조정\n",
        "print(len(y_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2lmINiu_YRT"
      },
      "outputs": [],
      "source": [
        "# KoBERT 학습모델 만들기\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=len(y_dict),   ##클래스 수 조정해줘야함##\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHb3aC0lDAN1"
      },
      "outputs": [],
      "source": [
        "# GPU 실행 오류 나면 사용\n",
        "# import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RREGFSjP_YdE"
      },
      "outputs": [],
      "source": [
        "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHkQBcNH_YlM"
      },
      "outputs": [],
      "source": [
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJIizFmW_Yqr"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxWCr7p1_Yu8"
      },
      "outputs": [],
      "source": [
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nosWVs3K_ZyS"
      },
      "outputs": [],
      "source": [
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYEmaX_B_bg2"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B7KpzAkJLlY"
      },
      "outputs": [],
      "source": [
        "# pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRNjVD4gBJMU"
      },
      "outputs": [],
      "source": [
        "highest_acc = 0\n",
        "patience = 0\n",
        "\n",
        "# 최종 모델 학습시키기\n",
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    for test_batch_id, (test_token_ids, test_valid_length, test_segment_ids, test_label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        test_token_ids = test_token_ids.long().to(device)\n",
        "        test_segment_ids = test_segment_ids.long().to(device)\n",
        "        test_valid_length= test_valid_length\n",
        "        test_label = test_label.long().to(device)\n",
        "        test_out = model(token_ids, valid_length, segment_ids)\n",
        "        test_loss = loss_fn(out, label)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (test_batch_id+1)))\n",
        "\n",
        "    if test_acc > highest_acc:\n",
        "        torch.save({\n",
        "            'epoch': e,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': test_loss,\n",
        "            }, path + 'final_data/' + 'correct_model_fin.pt')\n",
        "        patience = 0\n",
        "    else:\n",
        "        print(\"test acc did not improved. best:{} current:{}\".format(highest_acc, test_acc))\n",
        "        patience += 1\n",
        "        if patience > 5:\n",
        "            break\n",
        "    print('current patience: {}'.format(patience))\n",
        "    print(\"************************************************************************************\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSOz-5fU0x7k"
      },
      "outputs": [],
      "source": [
        "# # 학습한 모델 pickle 형태로 저장\n",
        "\n",
        "# import pickle\n",
        "# # path = '/content/drive/MyDrive/nlp_c/'\n",
        "\n",
        "# with open(path+'model_trial_fin_noclean.pickle', 'wb') as f:\n",
        "#     pickle.dump(model, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3rY76aBFora"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4a3DXQOusr0"
      },
      "source": [
        "# **코드북으로 제출형식 만들기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZR0j685k7pF"
      },
      "source": [
        "## 한국표준산업분류(10차)_국문 자료 이용해서 코드 북 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeQCABTf60KU"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ml_Po-T3jSBg"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/nlp_c/'\n",
        "code_book = pd.read_excel(path + '한국표준산업분류(10차)_국문.xlsx', header = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAR9qLt4R2HG"
      },
      "outputs": [],
      "source": [
        "code_book = code_book.dropna(subset = ['소분류(232)'])\n",
        "code_book[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzTN9XY2SAQV"
      },
      "outputs": [],
      "source": [
        "code = code_book[['대분류(21)', '중분류(77)', '소분류(232)', 'Unnamed: 5']][1:].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4SnceDhXlsA"
      },
      "outputs": [],
      "source": [
        "def na_to_code(data):\n",
        "  data_l = []\n",
        "  temp = data[0]\n",
        "\n",
        "  for i in range(0, len(data)):\n",
        "    if pd.isna(data[i]):\n",
        "      data[i] =  temp\n",
        "    else:\n",
        "      temp = data[i] \n",
        "    data_l.append(temp)\n",
        "  return data_l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2aNrSotg82A"
      },
      "outputs": [],
      "source": [
        "big = na_to_code(code.iloc[:,0].tolist())\n",
        "middle = na_to_code(code.iloc[:,1].tolist())\n",
        "small = na_to_code(code.iloc[:,2].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46LAfjWehPGJ"
      },
      "outputs": [],
      "source": [
        "code_b = pd.DataFrame(zip(big,middle,small), columns = ['big', 'middle', 'small'])\n",
        "code_b['y'] = code_b['small'].astype('int64')\n",
        "code_b['name'] = code['Unnamed: 5']\n",
        "code_b\n",
        "code_b.to_excel(path + 'codebook_dict.xlsx', index=False, encoding = 'EUC-KR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yW_jMgF4urpY"
      },
      "outputs": [],
      "source": [
        "code_b = pd.read_excel(path + 'codebook_dict.xlsx', dtype = {'big': str, 'middle': str, 'small': str})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN0K4p0Lu9hN"
      },
      "outputs": [],
      "source": [
        "code_b = code_b.iloc[:,:-1] # name은 참고용이므로 제외\n",
        "dict_fin = code_b.set_index('y').T.to_dict('list') # 소분류값을 key 로 한 dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ_h9PtlfN_m"
      },
      "source": [
        "## 학습한 모델 불러오기 및 환경 구축"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuRR8UlTmWuJ"
      },
      "outputs": [],
      "source": [
        "# # train 할 때, 메모리를 많이 사용하여, 비우기\n",
        "# import gc\n",
        "# gc.collect()\n",
        "# del model\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K60nnyCPRcz1"
      },
      "outputs": [],
      "source": [
        "# test 하기전 모델 기본 값 불러오기\n",
        "\n",
        "# KoBERT 입력 데이터로 만들기\n",
        "# BERT 모델에 들어가기 위한 dataset을 만들어주는 클래스\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))\n",
        "\n",
        "# 토큰화 실행\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "# KoBERT 학습모델 만들기\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=225,   ##클래스 수 조정해줘야함##\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9G82THgRcz1"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# # 학습한 model 열기\n",
        "\n",
        "# with open(path+'model_trial_fin_noclean.pickle', 'rb') as f:\n",
        "#     model = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rb1RPCCuFJBN"
      },
      "outputs": [],
      "source": [
        "# Setting parameters\n",
        "# 이건 나중에 최적화 값 찾아봐야할 듯\n",
        "max_len = 64\n",
        "batch_size = 128\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 6\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate = 5e-5 # 0.0001 # "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okNQY9oiFJBN"
      },
      "outputs": [],
      "source": [
        "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "\n",
        "checkpoint = torch.load(path + 'final_data/' + 'correct_model_fin.pt') # 학습한 파일 경로 지정\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDDJKVKuFJBN"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiPvR7nrQ8lZ"
      },
      "source": [
        "## 맞춤법 처리한 제출파일 제출 형식으로 바꾸기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrWKJ3ngnnUX"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv(path + 'final_data/' + 'correct_sub_fin.csv', encoding = 'utf-8-sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mETpKHS4Rcz1"
      },
      "outputs": [],
      "source": [
        "dataset_test = [[str(a), '0'] for a in test['clean_done']]\n",
        "dataset_test[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBesArAXRcz1"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFA8wsuvRcz2"
      },
      "outputs": [],
      "source": [
        "# 예측 함수 생성\n",
        "# Setting parameters\n",
        "# train 학습 모델 설정할 때와 동일하게 설정\n",
        "def predict_set(dataset_test):\n",
        "\n",
        "    test_acc = 0.0\n",
        "\n",
        "    tokenizer = get_tokenizer()\n",
        "    tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "    data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)\n",
        "\n",
        "    test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=4)\n",
        "\n",
        "    out_list =[]\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        output = out.detach().cpu().tolist()\n",
        "        out_list.append(output)\n",
        "\n",
        "    pd = sum(out_list,[])\n",
        "    pd_list = pd_list = [np.argmax(i) for i in pd]\n",
        "    return pd_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdY-aKjeRcz2"
      },
      "outputs": [],
      "source": [
        "p_test = predict_set(dataset_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V7ei8ZyqJOC"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# 학습하기전 기존 y값 사전 열기\n",
        "with open(path+'final_data/' + 's_dictionary', 'rb') as f:\n",
        "    s_dict = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maFRXRNgvHUk"
      },
      "outputs": [],
      "source": [
        "test['predict_y'] = p_test\n",
        "# 1. 모델 학습하기 전 기존 y값 변수로 변환\n",
        "test['predict_y'] = test['predict_y'].astype('str').map(s_dict) \n",
        "cols = ['digit_1', 'digit_2', 'digit_3']\n",
        "# 2. 코드북에서 소분류를 통해 대/중분류 함께 예측\n",
        "test[cols] = test['predict_y'].astype('int64').map(dict_fin).apply(lambda x: pd.Series(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeQrgHvTSbN3"
      },
      "outputs": [],
      "source": [
        "test_fin = test[['AI_id', 'digit_1', 'digit_2', 'digit_3', 'text_obj', 'text_mthd', 'text_deal']]\n",
        "test_fin.to_csv(path + 'final_data/' + 'submission_fin_0413.csv', index=False, encoding='EUC-KR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgVYTuLJ3yNG"
      },
      "outputs": [],
      "source": [
        "test_fin[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGGX37hj3yKv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "cdwBTlG6wKQf",
        "QMrFUhYp94rf",
        "S2fhaR9N-BsI",
        "zX5AkfxF-rOe",
        "ae89d4cd",
        "Yz6Emo9U4hsd",
        "fZR0j685k7pF",
        "hIDnWq7LmWrf",
        "A30HyWuF35dO",
        "-VMrKI0qyQu2"
      ],
      "machine_shape": "hm",
      "name": "code2_kobert_model_code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}